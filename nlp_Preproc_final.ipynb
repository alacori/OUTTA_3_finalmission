{"cells":[{"cell_type":"code","execution_count":17,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30905,"status":"ok","timestamp":1660750861283,"user":{"displayName":"오지민","userId":"10459316527126733779"},"user_tz":-540},"id":"4437c24d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting mxnet\n","  Downloading mxnet-1.9.1-py3-none-manylinux2014_x86_64.whl (49.1 MB)\n","\u001b[K     |████████████████████████████████| 49.1 MB 2.2 MB/s \n","\u001b[?25hCollecting graphviz\u003c0.9.0,\u003e=0.8.1\n","  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: numpy\u003c2.0.0,\u003e1.16.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (1.21.6)\n","Requirement already satisfied: requests\u003c3,\u003e=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (2.23.0)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3,\u003e=2.20.0-\u003emxnet) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3,\u003e=2.20.0-\u003emxnet) (1.24.3)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3,\u003e=2.20.0-\u003emxnet) (3.0.4)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3,\u003e=2.20.0-\u003emxnet) (2.10)\n","Installing collected packages: graphviz, mxnet\n","  Attempting uninstall: graphviz\n","    Found existing installation: graphviz 0.10.1\n","    Uninstalling graphviz-0.10.1:\n","      Successfully uninstalled graphviz-0.10.1\n","Successfully installed graphviz-0.8.4 mxnet-1.9.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting gluonnlp\n","  Downloading gluonnlp-0.10.0.tar.gz (344 kB)\n","\u001b[K     |████████████████████████████████| 344 kB 8.5 MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.0)\n","Requirement already satisfied: numpy\u003e=1.16.0 in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (1.21.6)\n","Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (0.29.32)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (21.3)\n","Requirement already satisfied: pytz\u003e=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n","Requirement already satisfied: python-dateutil\u003e=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil\u003e=2.7.3-\u003epandas) (1.15.0)\n","Requirement already satisfied: pyparsing!=3.0.5,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-\u003egluonnlp) (3.0.9)\n","Building wheels for collected packages: gluonnlp\n","  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp37-cp37m-linux_x86_64.whl size=595733 sha256=6b7b94b2c0643290e23c08603754aa3dda32d79d83b53b2727e2d6cb3847da7d\n","  Stored in directory: /root/.cache/pip/wheels/be/b4/06/7f3fdfaf707e6b5e98b79c041e023acffbe395d78a527eae00\n","Successfully built gluonnlp\n","Installing collected packages: gluonnlp\n","Successfully installed gluonnlp-0.10.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 8.0 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.97\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers==3.0.2\n","  Downloading transformers-3.0.2-py3-none-any.whl (769 kB)\n","\u001b[K     |████████████████████████████████| 769 kB 7.7 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (3.8.0)\n","Collecting tokenizers==0.8.1.rc1\n","  Downloading tokenizers-0.8.1rc1-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n","\u001b[K     |████████████████████████████████| 3.0 MB 53.3 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (21.3)\n","Requirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (4.64.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[K     |████████████████████████████████| 880 kB 57.8 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (1.21.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2022.6.2)\n","Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (0.1.97)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2.23.0)\n","Requirement already satisfied: pyparsing!=3.0.5,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-\u003etransformers==3.0.2) (3.0.9)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers==3.0.2) (2.10)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers==3.0.2) (2022.6.15)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers==3.0.2) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers==3.0.2) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers==3.0.2) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers==3.0.2) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers==3.0.2) (1.1.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=f2a4d65526d4bf92e4e26ad861c4c6b2ac9ee338c5885cc8c8c48bc5bea99d57\n","  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, transformers\n","Successfully installed sacremoses-0.0.53 tokenizers-0.8.1rc1 transformers-3.0.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.2+zzzcolab20220719082949)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem\u003e=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.26.0)\n","Requirement already satisfied: wrapt\u003e=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: gast\u003e=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n","Requirement already satisfied: flatbuffers\u003e=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n","Requirement already satisfied: typing-extensions\u003e=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.1.1)\n","Requirement already satisfied: grpcio\u003c2.0,\u003e=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.47.0)\n","Requirement already satisfied: keras\u003c2.9,\u003e=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n","Requirement already satisfied: keras-preprocessing\u003e=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n","Requirement already satisfied: absl-py\u003e=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.2.0)\n","Requirement already satisfied: h5py\u003e=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n","Requirement already satisfied: protobuf\u003c3.20,\u003e=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n","Requirement already satisfied: astunparse\u003e=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: tensorboard\u003c2.9,\u003e=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n","Requirement already satisfied: google-pasta\u003e=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: libclang\u003e=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.6)\n","Requirement already satisfied: numpy\u003e=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n","Requirement already satisfied: tensorflow-estimator\u003c2.9,\u003e=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n","Requirement already satisfied: opt-einsum\u003e=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: six\u003e=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n","Requirement already satisfied: termcolor\u003e=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n","Requirement already satisfied: wheel\u003c1.0,\u003e=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse\u003e=1.6.0-\u003etensorflow) (0.37.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py\u003e=2.9.0-\u003etensorflow) (1.5.2)\n","Requirement already satisfied: markdown\u003e=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003c2.9,\u003e=2.8-\u003etensorflow) (3.4.1)\n","Requirement already satisfied: google-auth\u003c3,\u003e=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003c2.9,\u003e=2.8-\u003etensorflow) (1.35.0)\n","Requirement already satisfied: werkzeug\u003e=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003c2.9,\u003e=2.8-\u003etensorflow) (1.0.1)\n","Requirement already satisfied: tensorboard-data-server\u003c0.7.0,\u003e=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003c2.9,\u003e=2.8-\u003etensorflow) (0.6.1)\n","Requirement already satisfied: tensorboard-plugin-wit\u003e=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003c2.9,\u003e=2.8-\u003etensorflow) (1.8.1)\n","Requirement already satisfied: requests\u003c3,\u003e=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003c2.9,\u003e=2.8-\u003etensorflow) (2.23.0)\n","Requirement already satisfied: google-auth-oauthlib\u003c0.5,\u003e=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003c2.9,\u003e=2.8-\u003etensorflow) (0.4.6)\n","Requirement already satisfied: pyasn1-modules\u003e=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003c2.9,\u003e=2.8-\u003etensorflow) (0.2.8)\n","Requirement already satisfied: rsa\u003c5,\u003e=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003c2.9,\u003e=2.8-\u003etensorflow) (4.9)\n","Requirement already satisfied: cachetools\u003c5.0,\u003e=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003c2.9,\u003e=2.8-\u003etensorflow) (4.2.4)\n","Requirement already satisfied: requests-oauthlib\u003e=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib\u003c0.5,\u003e=0.4.1-\u003etensorboard\u003c2.9,\u003e=2.8-\u003etensorflow) (1.3.1)\n","Requirement already satisfied: importlib-metadata\u003e=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown\u003e=2.6.8-\u003etensorboard\u003c2.9,\u003e=2.8-\u003etensorflow) (4.12.0)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata\u003e=4.4-\u003emarkdown\u003e=2.6.8-\u003etensorboard\u003c2.9,\u003e=2.8-\u003etensorflow) (3.8.1)\n","Requirement already satisfied: pyasn1\u003c0.5.0,\u003e=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules\u003e=0.2.1-\u003egoogle-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003c2.9,\u003e=2.8-\u003etensorflow) (0.4.8)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3,\u003e=2.21.0-\u003etensorboard\u003c2.9,\u003e=2.8-\u003etensorflow) (2022.6.15)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3,\u003e=2.21.0-\u003etensorboard\u003c2.9,\u003e=2.8-\u003etensorflow) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3,\u003e=2.21.0-\u003etensorboard\u003c2.9,\u003e=2.8-\u003etensorflow) (1.24.3)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3,\u003e=2.21.0-\u003etensorboard\u003c2.9,\u003e=2.8-\u003etensorflow) (3.0.4)\n","Requirement already satisfied: oauthlib\u003e=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib\u003e=0.7.0-\u003egoogle-auth-oauthlib\u003c0.5,\u003e=0.4.1-\u003etensorboard\u003c2.9,\u003e=2.8-\u003etensorflow) (3.2.0)\n"]}],"source":["!pip install mxnet\n","!pip install gluonnlp pandas tqdm\n","!pip install sentencepiece\n","!pip install transformers==3.0.2\n","!pip install torch\n","!pip install tensorflow"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1660750861284,"user":{"displayName":"오지민","userId":"10459316527126733779"},"user_tz":-540},"id":"cac208e4"},"outputs":[],"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import TensorDataset, DataLoader\n","import gluonnlp as nlp\n","import numpy as np\n","from tqdm import tqdm, tqdm_notebook\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import pandas as pd\n","import traceback"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":4879,"status":"ok","timestamp":1660750866154,"user":{"displayName":"오지민","userId":"10459316527126733779"},"user_tz":-540},"id":"47c7d1c6"},"outputs":[],"source":["!pip install --upgrade -q pyproj"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1660750866156,"user":{"displayName":"오지민","userId":"10459316527126733779"},"user_tz":-540},"id":"1076db8c"},"outputs":[],"source":["def mount_drive():\n","    from google.colab import drive\n","    drive.mount('/content/gdrive')\n","    %cd /content/gdrive/MyDrive/NLP"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3887,"status":"ok","timestamp":1660750870030,"user":{"displayName":"오지민","userId":"10459316527126733779"},"user_tz":-540},"id":"8b517415","outputId":"71688a96-c61b-4982-fa9c-1e04d1882d81"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","/content/gdrive/MyDrive/NLP\n"]}],"source":["if __name__ == '__main__':\n","  mount_drive()"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":3671,"status":"ok","timestamp":1660750873694,"user":{"displayName":"오지민","userId":"10459316527126733779"},"user_tz":-540},"id":"5a103ec9"},"outputs":[],"source":["if __name__ == '__main__':\n","  whole_dataset = pd.read_excel('data/chat_data.xlsx')\n","  whole_dataset.head()"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1660750873695,"user":{"displayName":"오지민","userId":"10459316527126733779"},"user_tz":-540},"id":"4e29fd60"},"outputs":[],"source":["def data_processing(raw_data):    \n","\n","    # 0: '공포', 1: '놀람', 2: '분노', 3: '슬픔', 4: '중립', 5: '행복, 6: '혐오'\n","    # Emotion 열에 있는 문자열을 대응되는 정수로 변환해주자\n","    # Hint : loc 함수를 활용하여 바꾸어 보자.\n","    ## 여기에 코드 작성\n","    raw_data.loc[raw_data['Emotion'] == '공포','Emotion'] = 0\n","    raw_data.loc[raw_data['Emotion'] == '놀람','Emotion'] = 1\n","    raw_data.loc[raw_data['Emotion'] == '분노','Emotion'] = 2\n","    raw_data.loc[raw_data['Emotion'] == '슬픔','Emotion'] = 3\n","    raw_data.loc[raw_data['Emotion'] == '중립','Emotion'] = 4\n","    raw_data.loc[raw_data['Emotion'] == '행복','Emotion'] = 5\n","    raw_data.loc[raw_data['Emotion'] == '혐오','Emotion'] = 6\n","\n","    # 판다스의 concat을 활용하여 'document' 데이터와 'label' 데이터를 연결해보자.\n","    # 연결한 데이터의 이름은 processed_data라고 하자.\n","    ## 여기에 코드 작성\n","    processed_data = raw_data\n","\n","    processed_data = pd.concat([raw_data['Sentence'], raw_data['Emotion']], axis=1)\n","\n","    processed_data.columns = ['sentence', 'label']\n","\n","    return processed_data"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1660750873696,"user":{"displayName":"오지민","userId":"10459316527126733779"},"user_tz":-540},"id":"694eee6f"},"outputs":[],"source":["def data_to_token_ids(tokenizer, single_sentence):\n","    # CLS 토큰과 SEP 토큰을 문장의 시작과 끝에 붙여보자.\n","    special_token_added = \"[CLS] \" + str(single_sentence) + \" [SEP]\"\n","    \n","    # KoBERTTokenizer의 tokenize 함수를 활용하여 문장을 토큰화해보자.\n","    tokenized_text = tokenizer.tokenize(special_token_added)\n","\n","    # KoBERTTokenizer의 convert_tokens_to_ids 함수를 활용하여 생성된 토큰을 숫자 형태로 바꿔주자.\n","    token_ids = [tokenizer.convert_tokens_to_ids(tokenized_text)]\n","\n","    MAX_LEN = 128\n","    # pad_sequences 함수를 활용하여 문장의 빈 칸에 padding을 넣어주자.\n","    token_ids_padded = pad_sequences(token_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","    token_ids_flatten = token_ids_padded.flatten()\n","    return token_ids_flatten"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":926,"status":"ok","timestamp":1660750874615,"user":{"displayName":"오지민","userId":"10459316527126733779"},"user_tz":-540},"id":"58ea495b"},"outputs":[],"source":["def token_ids_to_mask(token_ids):\n","    \n","    # token_id에서 0보다 큰 숫자만 유효하도록 하는 'mask' 리스트를 만들자.\n","    mask = [float(i\u003e0) for i in token_ids]\n","    \n","    return mask"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1660750874616,"user":{"displayName":"오지민","userId":"10459316527126733779"},"user_tz":-540},"id":"ca432eda"},"outputs":[],"source":["def tokenize_processed_data(tokenizer, processed_dataset):\n","    labels = processed_dataset['label'].to_numpy()\n","\n","    labels = labels.astype(np.int)\n","    \n","    # list comprehension을 활용하여 processed_dataset의 'sentence' 데이터를 id값으로 토큰화하자.\n","    tokenized_data = [data_to_token_ids(tokenizer, processed_data) for processed_data in processed_dataset['sentence']]\n","\n","    # list comprehension을 활용하여 앞서 토큰화한 데이터 id를 mask로 변환하자.\n","    attention_masks = [token_ids_to_mask(token_ids) for token_ids in tokenized_data]\n","    \n","    return tokenized_data, labels, attention_masks"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1660750874617,"user":{"displayName":"오지민","userId":"10459316527126733779"},"user_tz":-540},"id":"4a457193"},"outputs":[],"source":["def split_into_train_validation(whole_data, whole_label, whole_masks):\n","    print(\"length of whole_data : \" + str(len(whole_data)))\n","    \n","    # split_into_train_test의 코드를 참조하여 data와 mask를  train을 위한 것과 validation을 위한 것으로 나누자.\n","    \n","    train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(whole_data,\n","                                                                                    whole_label, \n","                                                                                    random_state=2022, \n","                                                                                    test_size=0.1)\n","    train_masks, validation_masks, _, _ = train_test_split(whole_masks, \n","                                                       whole_data,\n","                                                       random_state=2022, \n","                                                       test_size=0.1)\n","    \n","    print(\"length of train_data : \" + str(len(train_inputs)))\n","    \n","    return train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":581,"status":"ok","timestamp":1660750875192,"user":{"displayName":"오지민","userId":"10459316527126733779"},"user_tz":-540},"id":"f2934c79"},"outputs":[],"source":["def data_to_tensor(inputs, labels, masks):\n","    inputs_tensor = torch.tensor(inputs)\n","    labels_tensor = torch.tensor(labels)\n","    masks_tensor = torch.tensor(masks)\n","    return inputs_tensor, labels_tensor, masks_tensor"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1660750875193,"user":{"displayName":"오지민","userId":"10459316527126733779"},"user_tz":-540},"id":"731a2bb7"},"outputs":[],"source":["def tensor_to_dataloader(inputs, labels, masks, mode):\n","    from torch.utils.data import RandomSampler, SequentialSampler\n","    \n","    batch_size=32\n","    data = TensorDataset(inputs, masks, labels)\n","    \n","    if mode == \"train\":\n","        # train에 적합한 sampler을 지정하자.\n","        sampler = RandomSampler(data)\n","    else:\n","        # test에 적합한 sampler을 지정하자.\n","        sampler = SequentialSampler(data)\n","    \n","    dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n","    \n","    return dataloader"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1660750875194,"user":{"displayName":"오지민","userId":"10459316527126733779"},"user_tz":-540},"id":"de8e775f"},"outputs":[],"source":["def preproc(tokenizer, whole_dataset):\n","    # whole_dataset을 전처리하자.\n","    processed_dataset = data_processing(whole_dataset)\n","    \n","    # 전처리한 전체 데이터를 토큰화하자.\n","    tokenized_dataset, labels, attention_masks = tokenize_processed_data(tokenizer, processed_dataset)\n","\n","    # 토큰화한 train용 데이터를 train용과 validation용으로 분리하자.\n","    train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = split_into_train_validation(tokenized_dataset, labels, attention_masks)\n","\n","    # train용, validation용 데이터 각각을 텐서로 변환하자.\n","    train_inputs, train_labels, train_masks = data_to_tensor(train_inputs, train_labels, train_masks)\n","    validation_inputs, validation_labels, validation_masks = data_to_tensor(validation_inputs, validation_labels, validation_masks)\n","\n","    # train용, validation용 텐서를 dataloader로 변환하자. \n","    train_dataloader = tensor_to_dataloader(train_inputs, train_labels, train_masks, \"train\")\n","    validation_dataloader = tensor_to_dataloader(validation_inputs, validation_labels, validation_masks, \"validation\")\n","\n","    return train_dataloader, validation_dataloader"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1660750875194,"user":{"displayName":"오지민","userId":"10459316527126733779"},"user_tz":-540},"id":"8f52ec3c"},"outputs":[],"source":["def main():\n","    from nlp_tokenization import KoBertTokenizer\n","\n","    # 전체 데이터를 불러오자.\n","    whole_dataset = pd.read_excel('/content/gdrive/MyDrive/NLP/data/chat_data.xlsx')        \n","\n","    # KoBERTTokenizer를 불러오자.\n","    tokenizer = KoBertTokenizer.from_pretrained(\"monologg/kobert\")\n","  \n","    train, valid = preproc(tokenizer, whole_dataset)        "]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22524,"status":"ok","timestamp":1660750897709,"user":{"displayName":"오지민","userId":"10459316527126733779"},"user_tz":-540},"id":"a93ab059","outputId":"a889d2af-3d82-4201-adf3-c4beac274bae"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  after removing the cwd from sys.path.\n"]},{"name":"stdout","output_type":"stream","text":["length of whole_data : 34388\n","length of train_data : 30949\n"]}],"source":["if __name__ == '__main__':\n","    main()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"nlp_Preproc_final.ipynb","version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":5}