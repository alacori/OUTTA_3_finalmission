{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4437c24d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30905,
     "status": "ok",
     "timestamp": 1660750861283,
     "user": {
      "displayName": "오지민",
      "userId": "10459316527126733779"
     },
     "user_tz": -540
    },
    "id": "4437c24d",
    "outputId": "6638584b-c528-4d56-b626-510c031cf94e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: mxnet in /usr/local/lib/python3.7/dist-packages (1.9.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (1.21.6)\n",
      "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from mxnet) (0.8.4)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (2.23.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2022.6.15)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: gluonnlp in /usr/local/lib/python3.7/dist-packages (0.10.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (21.3)\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (0.29.32)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (1.21.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gluonnlp) (3.0.9)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.97)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: transformers==3.0.2 in /usr/local/lib/python3.7/dist-packages (3.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2.23.0)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (0.0.53)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (4.64.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (1.21.6)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2022.6.2)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (0.1.97)\n",
      "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (0.8.1rc1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.2) (3.0.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2022.6.15)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2.10)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.1.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.15.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.2+zzzcolab20220719082949)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.47.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.26.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.6)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
      "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.23.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.12.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.8.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2022.6.15)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mxnet\n",
    "!pip install gluonnlp pandas tqdm\n",
    "!pip install sentencepiece\n",
    "!pip install transformers==3.0.2\n",
    "!pip install torch\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cac208e4",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1660750861284,
     "user": {
      "displayName": "오지민",
      "userId": "10459316527126733779"
     },
     "user_tz": -540
    },
    "id": "cac208e4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47c7d1c6",
   "metadata": {
    "executionInfo": {
     "elapsed": 4879,
     "status": "ok",
     "timestamp": 1660750866154,
     "user": {
      "displayName": "오지민",
      "userId": "10459316527126733779"
     },
     "user_tz": -540
    },
    "id": "47c7d1c6"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade -q pyproj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1076db8c",
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1660750866156,
     "user": {
      "displayName": "오지민",
      "userId": "10459316527126733779"
     },
     "user_tz": -540
    },
    "id": "1076db8c"
   },
   "outputs": [],
   "source": [
    "def mount_drive():\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    %cd /content/gdrive/MyDrive/NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b517415",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3887,
     "status": "ok",
     "timestamp": 1660750870030,
     "user": {
      "displayName": "오지민",
      "userId": "10459316527126733779"
     },
     "user_tz": -540
    },
    "id": "8b517415",
    "outputId": "71688a96-c61b-4982-fa9c-1e04d1882d81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
      "/content/gdrive/MyDrive/NLP\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "  mount_drive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a103ec9",
   "metadata": {
    "executionInfo": {
     "elapsed": 3671,
     "status": "ok",
     "timestamp": 1660750873694,
     "user": {
      "displayName": "오지민",
      "userId": "10459316527126733779"
     },
     "user_tz": -540
    },
    "id": "5a103ec9"
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "  whole_dataset = pd.read_excel('data/chat_data.xlsx')\n",
    "  whole_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e29fd60",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1660750873695,
     "user": {
      "displayName": "오지민",
      "userId": "10459316527126733779"
     },
     "user_tz": -540
    },
    "id": "4e29fd60"
   },
   "outputs": [],
   "source": [
    "def data_processing(raw_data):    \n",
    "\n",
    "    # 0: '공포', 1: '놀람', 2: '분노', 3: '슬픔', 4: '중립', 5: '행복, 6: '혐오'\n",
    "    # Emotion 열에 있는 문자열을 대응되는 정수로 변환해주자\n",
    "    # Hint : loc 함수를 활용하여 바꾸어 보자.\n",
    "    ## 여기에 코드 작성\n",
    "    raw_data.loc[raw_data['Emotion'] == '공포','Emotion'] = 0\n",
    "    raw_data.loc[raw_data['Emotion'] == '놀람','Emotion'] = 1\n",
    "    raw_data.loc[raw_data['Emotion'] == '분노','Emotion'] = 2\n",
    "    raw_data.loc[raw_data['Emotion'] == '슬픔','Emotion'] = 3\n",
    "    raw_data.loc[raw_data['Emotion'] == '중립','Emotion'] = 4\n",
    "    raw_data.loc[raw_data['Emotion'] == '행복','Emotion'] = 5\n",
    "    raw_data.loc[raw_data['Emotion'] == '혐오','Emotion'] = 6\n",
    "\n",
    "    # 판다스의 concat을 활용하여 'document' 데이터와 'label' 데이터를 연결해보자.\n",
    "    # 연결한 데이터의 이름은 processed_data라고 하자.\n",
    "    ## 여기에 코드 작성\n",
    "    processed_data = raw_data\n",
    "\n",
    "    processed_data = pd.concat([raw_data['Sentence'], raw_data['Emotion']], axis=1)\n",
    "\n",
    "    processed_data.columns = ['sentence', 'label']\n",
    "\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "694eee6f",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1660750873696,
     "user": {
      "displayName": "오지민",
      "userId": "10459316527126733779"
     },
     "user_tz": -540
    },
    "id": "694eee6f"
   },
   "outputs": [],
   "source": [
    "def data_to_token_ids(tokenizer, single_sentence):\n",
    "    # CLS 토큰과 SEP 토큰을 문장의 시작과 끝에 붙여보자.\n",
    "    special_token_added = \"[CLS] \" + str(single_sentence) + \" [SEP]\"\n",
    "    \n",
    "    # KoBERTTokenizer의 tokenize 함수를 활용하여 문장을 토큰화해보자.\n",
    "    tokenized_text = tokenizer.tokenize(special_token_added)\n",
    "\n",
    "    # KoBERTTokenizer의 convert_tokens_to_ids 함수를 활용하여 생성된 토큰을 숫자 형태로 바꿔주자.\n",
    "    token_ids = [tokenizer.convert_tokens_to_ids(tokenized_text)]\n",
    "\n",
    "    MAX_LEN = 128\n",
    "    # pad_sequences 함수를 활용하여 문장의 빈 칸에 padding을 넣어주자.\n",
    "    token_ids_padded = pad_sequences(token_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "    token_ids_flatten = token_ids_padded.flatten()\n",
    "    return token_ids_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58ea495b",
   "metadata": {
    "executionInfo": {
     "elapsed": 926,
     "status": "ok",
     "timestamp": 1660750874615,
     "user": {
      "displayName": "오지민",
      "userId": "10459316527126733779"
     },
     "user_tz": -540
    },
    "id": "58ea495b"
   },
   "outputs": [],
   "source": [
    "def token_ids_to_mask(token_ids):\n",
    "    \n",
    "    # token_id에서 0보다 큰 숫자만 유효하도록 하는 'mask' 리스트를 만들자.\n",
    "    mask = [float(i>0) for i in token_ids]\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca432eda",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1660750874616,
     "user": {
      "displayName": "오지민",
      "userId": "10459316527126733779"
     },
     "user_tz": -540
    },
    "id": "ca432eda"
   },
   "outputs": [],
   "source": [
    "def tokenize_processed_data(tokenizer, processed_dataset):\n",
    "    labels = processed_dataset['label'].to_numpy()\n",
    "\n",
    "    labels = labels.astype(np.int)\n",
    "    \n",
    "    # list comprehension을 활용하여 processed_dataset의 'sentence' 데이터를 id값으로 토큰화하자.\n",
    "    tokenized_data = [data_to_token_ids(tokenizer, processed_data) for processed_data in processed_dataset['sentence']]\n",
    "\n",
    "    # list comprehension을 활용하여 앞서 토큰화한 데이터 id를 mask로 변환하자.\n",
    "    attention_masks = [token_ids_to_mask(token_ids) for token_ids in tokenized_data]\n",
    "    \n",
    "    return tokenized_data, labels, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a457193",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1660750874617,
     "user": {
      "displayName": "오지민",
      "userId": "10459316527126733779"
     },
     "user_tz": -540
    },
    "id": "4a457193"
   },
   "outputs": [],
   "source": [
    "def split_into_train_validation(whole_data, whole_label, whole_masks):\n",
    "    print(\"length of whole_data : \" + str(len(whole_data)))\n",
    "    \n",
    "    # split_into_train_test의 코드를 참조하여 data와 mask를  train을 위한 것과 validation을 위한 것으로 나누자.\n",
    "    \n",
    "    train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(whole_data,\n",
    "                                                                                    whole_label, \n",
    "                                                                                    random_state=2022, \n",
    "                                                                                    test_size=0.1)\n",
    "    train_masks, validation_masks, _, _ = train_test_split(whole_masks, \n",
    "                                                       whole_data,\n",
    "                                                       random_state=2022, \n",
    "                                                       test_size=0.1)\n",
    "    \n",
    "    print(\"length of train_data : \" + str(len(train_inputs)))\n",
    "    \n",
    "    return train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2934c79",
   "metadata": {
    "executionInfo": {
     "elapsed": 581,
     "status": "ok",
     "timestamp": 1660750875192,
     "user": {
      "displayName": "오지민",
      "userId": "10459316527126733779"
     },
     "user_tz": -540
    },
    "id": "f2934c79"
   },
   "outputs": [],
   "source": [
    "def data_to_tensor(inputs, labels, masks):\n",
    "    inputs_tensor = torch.tensor(inputs)\n",
    "    labels_tensor = torch.tensor(labels)\n",
    "    masks_tensor = torch.tensor(masks)\n",
    "    return inputs_tensor, labels_tensor, masks_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "731a2bb7",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1660750875193,
     "user": {
      "displayName": "오지민",
      "userId": "10459316527126733779"
     },
     "user_tz": -540
    },
    "id": "731a2bb7"
   },
   "outputs": [],
   "source": [
    "def tensor_to_dataloader(inputs, labels, masks, mode):\n",
    "    from torch.utils.data import RandomSampler, SequentialSampler\n",
    "    \n",
    "    batch_size=32\n",
    "    data = TensorDataset(inputs, masks, labels)\n",
    "    \n",
    "    if mode == \"train\":\n",
    "        # train에 적합한 sampler을 지정하자.\n",
    "        sampler = RandomSampler(data)\n",
    "    else:\n",
    "        # test에 적합한 sampler을 지정하자.\n",
    "        sampler = SequentialSampler(data)\n",
    "    \n",
    "    dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "de8e775f",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1660750875194,
     "user": {
      "displayName": "오지민",
      "userId": "10459316527126733779"
     },
     "user_tz": -540
    },
    "id": "de8e775f"
   },
   "outputs": [],
   "source": [
    "def preproc(tokenizer, whole_dataset):\n",
    "    # whole_dataset을 전처리하자.\n",
    "    processed_dataset = data_processing(whole_dataset)\n",
    "    \n",
    "    # 전처리한 전체 데이터를 토큰화하자.\n",
    "    tokenized_dataset, labels, attention_masks = tokenize_processed_data(tokenizer, processed_dataset)\n",
    "\n",
    "    # 토큰화한 train용 데이터를 train용과 validation용으로 분리하자.\n",
    "    train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = split_into_train_validation(tokenized_dataset, labels, attention_masks)\n",
    "\n",
    "    # train용, validation용 데이터 각각을 텐서로 변환하자.\n",
    "    train_inputs, train_labels, train_masks = data_to_tensor(train_inputs, train_labels, train_masks)\n",
    "    validation_inputs, validation_labels, validation_masks = data_to_tensor(validation_inputs, validation_labels, validation_masks)\n",
    "\n",
    "    # train용, validation용 텐서를 dataloader로 변환하자. \n",
    "    train_dataloader = tensor_to_dataloader(train_inputs, train_labels, train_masks, \"train\")\n",
    "    validation_dataloader = tensor_to_dataloader(validation_inputs, validation_labels, validation_masks, \"validation\")\n",
    "\n",
    "    return train_dataloader, validation_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f52ec3c",
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1660750875194,
     "user": {
      "displayName": "오지민",
      "userId": "10459316527126733779"
     },
     "user_tz": -540
    },
    "id": "8f52ec3c"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    from nlp_tokenization import KoBertTokenizer\n",
    "\n",
    "    # 전체 데이터를 불러오자.\n",
    "    whole_dataset = pd.read_excel('/content/gdrive/MyDrive/NLP/data/chat_data.xlsx')        \n",
    "\n",
    "    # KoBERTTokenizer를 불러오자.\n",
    "    tokenizer = KoBertTokenizer.from_pretrained(\"monologg/kobert\")\n",
    "  \n",
    "    train, valid = preproc(tokenizer, whole_dataset)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a93ab059",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22524,
     "status": "ok",
     "timestamp": 1660750897709,
     "user": {
      "displayName": "오지민",
      "userId": "10459316527126733779"
     },
     "user_tz": -540
    },
    "id": "a93ab059",
    "outputId": "a889d2af-3d82-4201-adf3-c4beac274bae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of whole_data : 34388\n",
      "length of train_data : 30949\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "nlp_Preproc_final.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
